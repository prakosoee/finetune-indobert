{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning F&B\n",
    "F&B is a Sentiment Analysis dataset with 3 possible labels: `positive`, `negative`, and `neutral`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\coding\\python\\indonlu\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "sys.path.append('../')\n",
    "os.chdir('../')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from utils.forward_fn import forward_sequence_classification\n",
    "from utils.metrics import document_sentiment_metrics_fn\n",
    "from utils.data_utils import DocumentSentimentDataset, DocumentSentimentDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# common functions\n",
    "###\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(26092020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "4.46.3\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(torch.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p2')\n",
    "config.num_labels = DocumentSentimentDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForSequenceClassification.from_pretrained('indobenchmark/indobert-base-p2', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124443651"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = r\"d:\\coding\\python\\indonlu\\dataset\\fnb\\food_quality\\train_preprocess.csv\"\n",
    "valid_dataset_path = r\"d:\\coding\\python\\indonlu\\dataset\\fnb\\food_quality\\valid_preprocess.csv\"\n",
    "test_dataset_path  = r\"d:\\coding\\python\\indonlu\\dataset\\fnb\\food_quality\\test_preprocess_masked_label.csv\"\n",
    "test_valid_dataset_path = r\"d:\\coding\\python\\indonlu\\dataset\\fnb\\food_quality\\test_preprocess.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DocumentSentimentDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = DocumentSentimentDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = DocumentSentimentDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "test_valid_dataset = DocumentSentimentDataset(test_valid_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = DocumentSentimentDataLoader(dataset=train_dataset, max_seq_len=128, batch_size=8, num_workers=0, shuffle=True)  \n",
    "valid_loader = DocumentSentimentDataLoader(dataset=valid_dataset, max_seq_len=128, batch_size=8, num_workers=0, shuffle=False)  \n",
    "test_loader = DocumentSentimentDataLoader(dataset=test_dataset, max_seq_len=128, batch_size=8, num_workers=0, shuffle=False)\n",
    "test_valid_loader = DocumentSentimentDataLoader(dataset=test_valid_dataset, max_seq_len=128, batch_size=8, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n",
      "{0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = DocumentSentimentDataset.LABEL2INDEX, DocumentSentimentDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Mie goreng terenak yang pernah saya makan tapi mahal dan pelayanan tidak bagus | Label : positive (36.708%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Mie goreng terenak yang pernah saya makan tapi mahal dan pelayanan tidak bagus'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: harganya murah banget tapi rasanya biasa aja | Label : positive (42.599%)\n"
     ]
    }
   ],
   "source": [
    "text = 'harganya murah banget tapi rasanya biasa aja'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: gak akan balik kesini lagi makanannya enak banget sampai buat sakit perut | Label : positive (43.699%)\n"
     ]
    }
   ],
   "source": [
    "text = 'gak akan balik kesini lagi makanannya enak banget sampai buat sakit perut'\n",
    "subwords = tokenizer.encode(text)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "\n",
    "logits = model(subwords)[0]\n",
    "label = torch.topk(logits, k=1, dim=-1)[1].squeeze().item()\n",
    "\n",
    "print(f'Text: {text} | Label : {i2w[label]} ({F.softmax(logits, dim=-1).squeeze()[label] * 100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "labels_train = train_dataset.labels\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\",\n",
    "    classes=np.array([0, 1, 2]),\n",
    "    y=np.array(labels_train)\n",
    ")\n",
    "\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model embedding device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(\"Model embedding device:\",\n",
    "      model.bert.embeddings.word_embeddings.weight.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.27it/s, loss=0.5556, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS: 0.5556 ACC:0.77 F1:0.77 REC:0.77 PRE:0.77 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 VALID: 100%|██████████| 59/59 [00:03<00:00, 19.44it/s, loss=0.1905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) VALID LOSS: 0.3778 ACC:0.85 F1:0.85 REC:0.85 PRE:0.85\n",
      "✔ Model terbaik disimpan (Epoch 1, VALID LOSS=0.3778)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 TRAIN: 100%|██████████| 273/273 [01:02<00:00,  4.36it/s, loss=0.2899, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS: 0.2899 ACC:0.90 F1:0.90 REC:0.90 PRE:0.90 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 VALID: 100%|██████████| 59/59 [00:02<00:00, 19.87it/s, loss=0.1964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) VALID LOSS: 0.3894 ACC:0.85 F1:0.85 REC:0.86 PRE:0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.29it/s, loss=0.1315, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS: 0.1315 ACC:0.96 F1:0.96 REC:0.96 PRE:0.96 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 VALID: 100%|██████████| 59/59 [00:02<00:00, 19.70it/s, loss=0.2494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) VALID LOSS: 0.4947 ACC:0.84 F1:0.83 REC:0.84 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.30it/s, loss=0.0961, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS: 0.0961 ACC:0.97 F1:0.97 REC:0.97 PRE:0.97 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 VALID: 100%|██████████| 59/59 [00:02<00:00, 19.68it/s, loss=0.2922]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) VALID LOSS: 0.5795 ACC:0.83 F1:0.83 REC:0.83 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.29it/s, loss=0.0671, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS: 0.0671 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 VALID: 100%|██████████| 59/59 [00:03<00:00, 19.53it/s, loss=0.3864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) VALID LOSS: 0.7663 ACC:0.81 F1:0.81 REC:0.82 PRE:0.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.28it/s, loss=0.0598, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS: 0.0598 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 VALID: 100%|██████████| 59/59 [00:03<00:00, 19.66it/s, loss=0.3216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) VALID LOSS: 0.6378 ACC:0.85 F1:0.85 REC:0.85 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.27it/s, loss=0.0359, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS: 0.0359 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 VALID: 100%|██████████| 59/59 [00:02<00:00, 19.82it/s, loss=0.3822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) VALID LOSS: 0.7579 ACC:0.84 F1:0.84 REC:0.85 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.31it/s, loss=0.0462, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS: 0.0462 ACC:0.98 F1:0.98 REC:0.98 PRE:0.98 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 VALID: 100%|██████████| 59/59 [00:03<00:00, 19.29it/s, loss=0.3064]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) VALID LOSS: 0.6077 ACC:0.85 F1:0.85 REC:0.85 PRE:0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 TRAIN: 100%|██████████| 273/273 [01:04<00:00,  4.25it/s, loss=0.0363, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS: 0.0363 ACC:0.99 F1:0.99 REC:0.99 PRE:0.99 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 VALID: 100%|██████████| 59/59 [00:02<00:00, 19.74it/s, loss=0.3297]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) VALID LOSS: 0.6538 ACC:0.84 F1:0.84 REC:0.84 PRE:0.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 TRAIN: 100%|██████████| 273/273 [01:03<00:00,  4.31it/s, loss=0.0151, lr=2.00e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS: 0.0151 ACC:1.00 F1:1.00 REC:1.00 PRE:1.00 LR:2.00e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 VALID: 100%|██████████| 59/59 [00:02<00:00, 19.89it/s, loss=0.4321]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) VALID LOSS: 0.8568 ACC:0.84 F1:0.84 REC:0.84 PRE:0.85\n",
      "\n",
      "Training selesai.\n",
      "Model terbaik tersimpan di folder: model_foood\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# =========================\n",
    "# KONFIGURASI\n",
    "# =========================\n",
    "n_epochs = 10\n",
    "save_dir = \"model_foood\"   # ganti sesuai aspek\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "# =========================\n",
    "# TRAINING LOOP\n",
    "# =========================\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    # =========================\n",
    "    # TRAINING\n",
    "    # =========================\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    train_preds, train_labels = [], []\n",
    "\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1} TRAIN\")\n",
    "\n",
    "    for batch_data in train_pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss, batch_pred, batch_label = forward_sequence_classification(\n",
    "            model=model,\n",
    "            batch_data=batch_data,\n",
    "            device=device,\n",
    "            criterion=criterion\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        train_preds.extend(batch_pred)\n",
    "        train_labels.extend(batch_label)\n",
    "\n",
    "        train_pbar.set_postfix({\n",
    "            \"loss\": f\"{total_train_loss / (len(train_preds)//len(batch_pred)):.4f}\",\n",
    "            \"lr\": f\"{get_lr(optimizer):.2e}\"\n",
    "        })\n",
    "\n",
    "    train_metrics = document_sentiment_metrics_fn(train_preds, train_labels)\n",
    "\n",
    "    print(\n",
    "        f\"(Epoch {epoch+1}) \"\n",
    "        f\"TRAIN LOSS: {total_train_loss / len(train_loader):.4f} \"\n",
    "        f\"{metrics_to_string(train_metrics)} \"\n",
    "        f\"LR:{get_lr(optimizer):.2e}\"\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # VALIDATION\n",
    "    # =========================\n",
    "    model.eval()\n",
    "    total_valid_loss = 0\n",
    "    valid_preds, valid_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_pbar = tqdm(valid_loader, desc=f\"Epoch {epoch+1} VALID\")\n",
    "\n",
    "        for batch_data in valid_pbar:\n",
    "            loss, batch_pred, batch_label = forward_sequence_classification(\n",
    "                model=model,\n",
    "                batch_data=batch_data,\n",
    "                device=device,\n",
    "            criterion=criterion\n",
    "            )\n",
    "\n",
    "            total_valid_loss += loss.item()\n",
    "            valid_preds.extend(batch_pred)\n",
    "            valid_labels.extend(batch_label)\n",
    "\n",
    "            valid_pbar.set_postfix({\n",
    "                \"loss\": f\"{total_valid_loss / (len(valid_preds)//len(batch_pred)):.4f}\"\n",
    "            })\n",
    "\n",
    "    avg_valid_loss = total_valid_loss / len(valid_loader)\n",
    "    valid_metrics = document_sentiment_metrics_fn(valid_preds, valid_labels)\n",
    "\n",
    "    print(\n",
    "        f\"(Epoch {epoch+1}) \"\n",
    "        f\"VALID LOSS: {avg_valid_loss:.4f} \"\n",
    "        f\"{metrics_to_string(valid_metrics)}\"\n",
    "    )\n",
    "\n",
    "    # =========================\n",
    "    # SIMPAN MODEL TERBAIK\n",
    "    # =========================\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "\n",
    "        print(\n",
    "            f\"✔ Model terbaik disimpan \"\n",
    "            f\"(Epoch {epoch+1}, VALID LOSS={best_valid_loss:.4f})\"\n",
    "        )\n",
    "\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(\"\\nTraining selesai.\")\n",
    "print(f\"Model terbaik tersimpan di folder: {save_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
