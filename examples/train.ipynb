{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6749cc4",
   "metadata": {},
   "source": [
    " # Tahap Tokenisasi dengan IndoBERT Tokenizer\n",
    "\n",
    "Tahap ini mengubah teks menjadi format yang bisa dibaca model (token IDs, attention mask, dll).\n",
    "\n",
    "**Yang dilakukan:**\n",
    "- Load tokenizer dari model IndoBERT base-p2\n",
    "- Tokenisasi kolom 'text' pada train, validation, dan test set\n",
    "- Gunakan max_length=512 (standar untuk base model)\n",
    "- Simpan hasil tokenisasi supaya siap untuk training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4890fc0",
   "metadata": {},
   "source": [
    "**IMPORT LIBRARY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e13dd7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\coding\\python\\indonlu\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     11\u001b[0m     AutoTokenizer,\n\u001b[0;32m     12\u001b[0m     AutoModelForSequenceClassification,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     get_linear_schedule_with_warmup\n\u001b[0;32m     16\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from accelerate import Accelerator\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from torch.optim import AdamW\n",
    "\n",
    "print(\"Semua library berhasil diimport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e291e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file split\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_df = pd.read_csv(\"../dataset/aes_2/train.csv\")\n",
    "valid_df = pd.read_csv(\"../dataset/aes_2/valid.csv\")\n",
    "test_df  = pd.read_csv(\"../dataset/aes_2/test.csv\")\n",
    "\n",
    "# Ubah ke HuggingFace dataset\n",
    "final_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(valid_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "print(\"Dataset berhasil dimuat:\")\n",
    "print(final_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d7653c",
   "metadata": {},
   "source": [
    "**SET SEED DAN KONSTANTA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520df31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi set seed\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # kalau nanti pakai GPU\n",
    "\n",
    "# Jalankan sekali di awal\n",
    "set_seed(42)\n",
    "\n",
    "# Konstanta utama\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p1\"\n",
    "PROCESSED_FILE = \"../data/processed/aes_preprocessed.csv\"\n",
    "OUTPUT_DIR = \"./results_aes\"\n",
    "\n",
    "MAX_LENGTH = 256\n",
    "TRAIN_BATCH_SIZE = 2\n",
    "EVAL_BATCH_SIZE = 2\n",
    "\n",
    "# Epoch dinamis\n",
    "MAX_EPOCHS = 5          # batas maksimum\n",
    "EARLY_STOPPING_PATIENCE = 2  # berhenti jika 2 epoch tidak membaik\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_STEPS = 200\n",
    "\n",
    "print(\"Seed sudah diset & konstanta sudah didefinisikan\")\n",
    "print(f\"Model yang dipakai: {MODEL_NAME}\")\n",
    "print(f\"File data: {PROCESSED_FILE}\")\n",
    "print(f\"Maksimum epoch: {MAX_EPOCHS} (dengan early stopping)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203bb8b6",
   "metadata": {},
   "source": [
    "Tahap Tokenisasi dengan IndoBERT Tokenizer\n",
    "\n",
    "Setelah data sudah di-split, kita ubah teks menjadi format token yang bisa dipahami model.\n",
    "\n",
    "**Yang dilakukan:**\n",
    "- Load tokenizer dari IndoBERT base-p2\n",
    "- Tokenisasi kolom 'text' pada train & validation (untuk training)\n",
    "- Tokenisasi test set terpisah (untuk evaluasi akhir)\n",
    "- Gunakan max_length=512 (standar base model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61874816",
   "metadata": {},
   "source": [
    "**LOAD TOKENIZER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13374a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nama model yang kita pakai\n",
    "MODEL_NAME = \"indobenchmark/indobert-base-p2\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"Memuat tokenizer {MODEL_NAME}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(\"Tokenizer berhasil dimuat\")\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Max length default:\", tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572e8ec8",
   "metadata": {},
   "source": [
    "**FUNGSI TOKENISASI & JALANKAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a87fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi tokenisasi (Soal + Jawaban)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"Soal\"],\n",
    "        examples[\"Jawaban\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "# Tokenisasi train & validation\n",
    "print(\"Tokenisasi train & validation...\")\n",
    "\n",
    "tokenized_train = final_dataset[\"train\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"Soal\", \"Jawaban\"]\n",
    ")\n",
    "\n",
    "tokenized_valid = final_dataset[\"validation\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"Soal\", \"Jawaban\"]\n",
    ")\n",
    "\n",
    "# Tokenisasi test\n",
    "print(\"Tokenisasi test...\")\n",
    "tokenized_test = final_dataset[\"test\"].map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"Soal\", \"Jawaban\"]\n",
    ")\n",
    "\n",
    "print(\"Tokenisasi selesai!\")\n",
    "print(\"Contoh satu sampel train setelah tokenisasi:\")\n",
    "print(tokenized_train[0])\n",
    "\n",
    "# ==============================\n",
    "# TABEL CONTOH TOKEN DAN ID\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "\n",
    "sample_ids = tokenized_train[0][\"input_ids\"]\n",
    "sample_tokens = tokenizer.convert_ids_to_tokens(sample_ids)\n",
    "\n",
    "# Ambil 15 token pertama agar tabel tidak terlalu panjang\n",
    "df_tokens = pd.DataFrame({\n",
    "    \"Token\": sample_tokens[:15],\n",
    "    \"ID\": sample_ids[:15]\n",
    "})\n",
    "\n",
    "print(\"\\nContoh tabel tokenisasi:\")\n",
    "display(df_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc08a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abcca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = tokenized_train.rename_column(\"label\", \"labels\")\n",
    "tokenized_valid = tokenized_valid.rename_column(\"label\", \"labels\")\n",
    "tokenized_test  = tokenized_test.rename_column(\"label\", \"labels\")\n",
    "\n",
    "print(\"Kolom setelah rename:\")\n",
    "print(tokenized_train.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b286c8a6",
   "metadata": {},
   "source": [
    "**SET FORMAT PYTORCH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eb8ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set format supaya jadi tensor PyTorch\n",
    "tokenized_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_valid.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "print(\"Format PyTorch sudah diset\")\n",
    "print(\"Kolom yang tersedia sekarang:\", tokenized_train.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5f093",
   "metadata": {},
   "source": [
    "# Tahap Load Model & Setup Training\n",
    "\n",
    "Sekarang data sudah ditokenisasi, kita:\n",
    "1. Load model IndoBERT base-p2 untuk tugas regression (num_labels=1)\n",
    "2. Setup TrainingArguments (hyperparameter CPU-friendly + early stopping)\n",
    "3. Setup Trainer dengan compute_metrics RMSE (sesuai permintaan dosen)\n",
    "4. Jalankan training\n",
    "\n",
    "**Catatan:**  \n",
    "- Epoch dibuat dinamis via early stopping  \n",
    "- Batch size aman untuk CPU + RAM 8 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a718e5",
   "metadata": {},
   "source": [
    "**Load Model IndoBERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9442c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Tentukan device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load model IndoBERT untuk regression\n",
    "print(f\"Memuat model {MODEL_NAME}...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=1,                   # regression → output 1 nilai\n",
    "    problem_type=\"regression\"       # pakai loss MSE otomatis\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model berhasil dimuat\")\n",
    "print(\"Device:\", device)\n",
    "print(\"Jumlah parameter:\", model.num_parameters())\n",
    "\n",
    "# =================================\n",
    "# CONTOH PREDIKSI AWAL MODEL\n",
    "# =================================\n",
    "# Ambil satu sampel dari data train\n",
    "sample = tokenized_train[0]\n",
    "\n",
    "# Siapkan input dasar\n",
    "inputs = {\n",
    "    \"input_ids\": sample[\"input_ids\"].unsqueeze(0).to(device),\n",
    "    \"attention_mask\": sample[\"attention_mask\"].unsqueeze(0).to(device),\n",
    "}\n",
    "\n",
    "# Tambahkan token_type_ids hanya jika tersedia\n",
    "if \"token_type_ids\" in sample:\n",
    "    inputs[\"token_type_ids\"] = sample[\"token_type_ids\"].unsqueeze(0).to(device)\n",
    "\n",
    "# Prediksi sebelum training\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(**inputs)\n",
    "\n",
    "prediksi = output.logits.item()\n",
    "label_asli = sample[\"labels\"].item()\n",
    "\n",
    "print(\"\\nContoh prediksi awal model:\")\n",
    "print(\"Prediksi (0–1) :\", prediksi)\n",
    "print(\"Label asli (0–1):\", label_asli)\n",
    "\n",
    "# Versi skala asli 1–10\n",
    "print(\"\\nDalam skala 1–10:\")\n",
    "print(\"Prediksi :\", prediksi * 10)\n",
    "print(\"Label    :\", label_asli * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c540bf",
   "metadata": {},
   "source": [
    "**TrainingArguments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4faefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=MAX_EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "\n",
    "    gradient_accumulation_steps=4,   # tambahkan ini\n",
    "\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "\n",
    "    dataloader_num_workers=0, \n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efc7caa",
   "metadata": {},
   "source": [
    "**DEFINE COMPUTE METRICS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecde1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics hanya RMSE (sesuai dosen)\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    rmse = np.sqrt(mean_squared_error(labels, predictions))\n",
    "    return {\"eval_rmse\": rmse}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e71ef84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7ac3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_valid,\n",
    "    compute_metrics=compute_metrics,        # hanya RMSE\n",
    ")\n",
    "\n",
    "print(\"Trainer siap!\")\n",
    "print(\"Mulai training... (bisa memakan waktu beberapa jam di CPU)\")\n",
    "\n",
    "# Jalankan training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd01f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi di test set\n",
    "print(\"Evaluasi akhir di test set:\")\n",
    "test_results = trainer.evaluate(tokenized_test)\n",
    "print(test_results)\n",
    "\n",
    "# Tampilkan RMSE akhir\n",
    "print(f\"\\nRMSE di test set: {test_results['eval_rmse']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592df94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history = trainer.state.log_history\n",
    "\n",
    "history_train_loss = []\n",
    "history_valid_rmse = []\n",
    "\n",
    "for log in history:\n",
    "    if \"loss\" in log and \"epoch\" in log:\n",
    "        history_train_loss.append(log[\"loss\"])\n",
    "    if \"eval_rmse\" in log:\n",
    "        history_valid_rmse.append(log[\"eval_rmse\"])\n",
    "\n",
    "print(\"Train loss:\", history_train_loss)\n",
    "print(\"Valid RMSE:\", history_valid_rmse)\n",
    "\n",
    "# Cek jika kosong\n",
    "if len(history_train_loss) == 0 or len(history_valid_rmse) == 0:\n",
    "    print(\"Data history kosong. Jalankan trainer.train() dulu.\")\n",
    "else:\n",
    "    min_len = min(len(history_train_loss), len(history_valid_rmse))\n",
    "    history_train_loss = history_train_loss[:min_len]\n",
    "    history_valid_rmse = history_valid_rmse[:min_len]\n",
    "    history_train_rmse = [loss ** 0.5 for loss in history_train_loss]\n",
    "\n",
    "    epochs_range = range(1, min_len + 1)\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs_range, history_train_rmse, marker=\"o\", label=\"Train RMSE\")\n",
    "    plt.plot(epochs_range, history_valid_rmse, marker=\"o\", label=\"Valid RMSE\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.title(\"Train vs Validation RMSE\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
